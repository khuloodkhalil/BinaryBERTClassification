{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an engagemnt classifier\n",
    "\n",
    "In this notebook, we will build a simple, fast, and accurate text classification model in 3 simple steps. We will build a binary BERT model that classifies any text problem you would like to solve as either high or low.\n",
    "\n",
    "You need to use your own dataset, because of privacy issues I can't share the social media data I'm using in this tutorial.\n",
    "\n",
    "Each entry in the dataset should includes 'text' and a targeted measure that you want to predict. In my case I'm using the Instagram post as the text and my target measue is an engagement metric which is the number of likes for the post.  The target measure will be converted into binary classification by assigning posts with a the 50% top likes a high label of 1 and assigning the bottom 50% posts with likes a low label of 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Import the needed libraries\n",
    "\n",
    "# importing ktrain\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "# importing tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "seed_value=13\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "seed_value=13\n",
    "classes_number=2\n",
    "engagement_metric='likes'#The neam of the targeted measure in my dataset\n",
    "\n",
    "# For reading the datafile.. make sure to add a valid file path if not located in the same folder as the python code\n",
    "file_name='yourDataFile.csv'#you need to change this for your data file csv file with text and likes fields\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean data\n",
    "# Thanks for this repository where I used the clean function\n",
    "# https://github.com/Hind-Almerekhi/toxicityChangesReddit/blob/main/Classification/fineTunedBERT.ipynb\n",
    "\n",
    "def clean(text, newline=True, quote=True, bullet_point=True, \n",
    "          link=True, strikethrough=True, spoiler=True,\n",
    "          code=True, superscript=True, table=True, heading=True):\n",
    "    \"\"\"\n",
    "    Cleans text (string).\n",
    "      * \\n (newlines)\n",
    "      * &gt; (> quotes)\n",
    "      * * or &amp;#x200B; (bullet points)\n",
    "      * []() (links)\n",
    "      * etc (see below)\n",
    "    Specific removals can be turned off, but everything is on by default.\n",
    "    Standard punctuation etc is deliberately not removed, can be done in a\n",
    "    second round manually, or may be preserved in any case.\n",
    "    \"\"\"\n",
    "    # Newlines (replaced with space to preserve cases like word1\\nword2)\n",
    "    text = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                          \" \",          # Replace all non-letters with spaces\n",
    "                          str(text))\n",
    "    \n",
    "    if newline:\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "        # Remove resulting ' '\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "\n",
    "    # > Quotes\n",
    "    if quote:\n",
    "        text = re.sub(r'\\\"?\\\\?&?gt;?', '', text)\n",
    "\n",
    "    # Bullet points/asterisk (bold/italic)\n",
    "    if bullet_point:\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        text = re.sub('&amp;#x200B;', '', text)\n",
    "\n",
    "    # []() Link (Also removes the hyperlink)\n",
    "    if link:\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Strikethrough\n",
    "    if strikethrough:\n",
    "        text = re.sub('~', '', text)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    if spoiler:\n",
    "        text = re.sub('&lt;', '', text)\n",
    "        text = re.sub(r'!(.*?)!', r'\\1', text)\n",
    "\n",
    "    # Code, inline and block\n",
    "    if code:\n",
    "        text = re.sub('`', '', text)\n",
    "\n",
    "    # Superscript (Preserves the text)\n",
    "    if superscript:\n",
    "        text = re.sub(r'\\^\\((.*?)\\)', r'\\1', text)\n",
    "\n",
    "    # Table\n",
    "    if table:\n",
    "        text = re.sub(r'\\|', ' ', text)\n",
    "        text = re.sub(':-', '', text)\n",
    "\n",
    "    # Heading\n",
    "    if heading:\n",
    "        text = re.sub('#', '', text)\n",
    "    return text\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1:  Loading and Preprocessing the Dataset\n",
    "We set `val_pct` as 0.2, which will automatically sample 20% of the data for validation.  We specifiy `preprocess_mode='bert'`, as we will fine-tuning a BERT model in this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the datafile.. \n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "df = df.sample(frac=1,random_state=seed_value)\n",
    "df['text'] = df['text'].apply(lambda x: clean(x))\n",
    "\n",
    "\n",
    "# Remove outliers, the top and bottom 1% of the records\n",
    "q_low = df[engagement_metric].quantile(0.01)\n",
    "q_hi  = df[engagement_metric].quantile(0.99)\n",
    "df = df[(df[engagement_metric] < q_hi) & (df[engagement_metric] > q_low)]\n",
    "\n",
    "\n",
    "# labeling the data based on the quantile value \n",
    "# the top 50% of the records are labeled 1, and the bottom 50% records are labeles 0\n",
    "df[engagement_metric]=pd.qcut(df[engagement_metric], classes_number, labels=list(range(0,classes_number)))\n",
    "\n",
    "# only keeping the record we need from the dataframe.\n",
    "df = df[[engagement_metric, 'text']] \n",
    "\n",
    "# labeling the target with high and low. Through converting 1 to 'high' and 0 to 'low'\n",
    "df[engagement_metric] = df[engagement_metric].apply(lambda x: 'low' if x <1 else 'high')\n",
    "\n",
    "# Renaming the colums to label and text\n",
    "df.columns = ['label', 'text'] \n",
    "print(df.head())\n",
    "\n",
    "# Here we need to have separate colums for each label, so we end up having 3 colums for the training dataset \n",
    "# (text.low,high) important format for the BERT binary model\n",
    "df = pd.concat([df, df.label.astype('str').str.get_dummies()], axis=1, sort=False)\n",
    "df = df[['text', 'low', 'high']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data into training and testing (20%)\n",
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(df, \n",
    "                                                                   'text', # the column containing text\n",
    "                                                                   label_columns=['low', 'high'],#The labels\n",
    "                                                                   maxlen=256, \n",
    "                                                                   max_features=100000,\n",
    "                                                                   preprocess_mode='bert',\n",
    "                                                                   val_pct=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:  Creating the BERT Model and Wraping in Learner Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ a neural implementation of the [NBSVM](https://www.aclweb.org/anthology/P12-2018/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the BERT model using text_classifier \n",
    "model = text.text_classifier('bert', (x_train, y_train) , preproc=preproc)\n",
    "learner = ktrain.get_learner(model, \n",
    "                             train_data=(x_train, y_train), \n",
    "                             val_data=(x_test, y_test), \n",
    "                             batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Training the BERT Model\n",
    "We will use the `fit_onecycle` method that employs a 1cycle learning rate (5e-5) policy and train 4 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_onecycle(5e-5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()#validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ktrain.get_predictor(learner.model, preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting label for:\n",
    "> \"*#Austria’s supreme court has dropped its terrorism case against university professor #FaridHafez after an Al Jazeera documentary revealed charges were based on false evidence and fabricated accusations. We spoke to him about the case.\".*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.predict(\"#Austria’s supreme court has dropped its terrorism case against university professor #FaridHafez after an Al Jazeera documentary revealed charges were based on false evidence and fabricated accusations. We spoke to him about the case.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our Predictor for Later Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for later use\n",
    "p.save(f'Trained_BERT_model_4epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from disk\n",
    "p_saved = ktrain.load_predictor('Trained_BERT_model_4epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using the saved model after loading\n",
    "p_saved.predict(\"#Austria’s supreme court has dropped its terrorism case against university professor #FaridHafez after an Al Jazeera documentary revealed charges were based on false evidence and fabricated accusations. We spoke to him about the case.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
